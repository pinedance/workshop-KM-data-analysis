{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization ( or Segmentation )\n",
    "\n",
    "텍스트는 문자(`chr`)의 나열(`string`)이다. 이를 데이터로 사용하기 위해서는 기본 단위로 분절해야 한다. \n",
    "\n",
    "이렇게 분절된 결과를 __token__이라고 하고, 이런 과정을 _tokenization_이라고 한다. \n",
    "\n",
    "때때로 __Segmentation__이라고도 하는데, 한문과 같이 하나의 긴 글자의 나열을 적절하게 끊는다는 의미로 사용된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram \n",
    "\n",
    "문자의 나열이므로 적당한 문자 길이로 자르는 방법을 떠올릴 수 있다. \n",
    "\n",
    "한문의 경우에는 한 글자가 한 단어의 의미를 가지므로 uni-gram도 이용할 수 있다. \n",
    "\n",
    "한의학의 단어들은 2-6글자 사이이므로 bi-gram을 이용해도 어느정도의 특징을 분석해 낼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雜病篇卷之四 > 內傷 > 勞倦傷治法 > 補中益氣湯 1.17.1\n",
    "# https://mediclassics.kr/books/8/volume/12#content_267\n",
    "data1 = \"治勞役太甚或飮食失節身熱而煩自汗倦怠黃芪 一錢半人參白朮甘草各一錢當歸身陳皮各五分升麻柴胡各三分右剉作一貼水煎服\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# N-gram \n",
      "* Uni-gram: ['治', '勞', '役', '太', '甚', '或', '飮', '食', '失', '節', '身', '熱', '而', '煩', '自', '汗', '倦', '怠', '黃', '芪', ' ', '一', '錢', '半', '人', '參', '白', '朮', '甘', '草', '各', '一', '錢', '當', '歸', '身', '陳', '皮', '各', '五', '分', '升', '麻', '柴', '胡', '各', '三', '分', '右', '剉', '作', '一', '貼', '水', '煎', '服']\n"
     ]
    }
   ],
   "source": [
    "# 1gram\n",
    "uni_gram = list( data1 )\n",
    "print( \"# N-gram \")\n",
    "print( \"* Uni-gram:\", uni_gram )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Bi-gram: ['治勞', '勞役', '役太', '太甚', '甚或', '或飮', '飮食', '食失', '失節', '節身', '身熱', '熱而', '而煩', '煩自', '自汗', '汗倦', '倦怠', '怠黃', '黃芪', '芪 ', ' 一', '一錢', '錢半', '半人', '人參', '參白', '白朮', '朮甘', '甘草', '草各', '各一', '一錢', '錢當', '當歸', '歸身', '身陳', '陳皮', '皮各', '各五', '五分', '分升', '升麻', '麻柴', '柴胡', '胡各', '各三', '三分', '分右', '右剉', '剉作', '作一', '一貼', '貼水', '水煎', '煎服']\n",
      "* Tri-gram: ['治勞役', '勞役太', '役太甚', '太甚或', '甚或飮', '或飮食', '飮食失', '食失節', '失節身', '節身熱', '身熱而', '熱而煩', '而煩自', '煩自汗', '自汗倦', '汗倦怠', '倦怠黃', '怠黃芪', '黃芪 ', '芪 一', ' 一錢', '一錢半', '錢半人', '半人參', '人參白', '參白朮', '白朮甘', '朮甘草', '甘草各', '草各一', '各一錢', '一錢當', '錢當歸', '當歸身', '歸身陳', '身陳皮', '陳皮各', '皮各五', '各五分', '五分升', '分升麻', '升麻柴', '麻柴胡', '柴胡各', '胡各三', '各三分', '三分右', '分右剉', '右剉作', '剉作一', '作一貼', '一貼水', '貼水煎', '水煎服']\n"
     ]
    }
   ],
   "source": [
    "def n_gram( text, n=2 ):\n",
    "    size = len( text )\n",
    "    grams = [ text[i:i+n] for i in range(size -n+1 ) ]\n",
    "    return grams\n",
    "\n",
    "bi_gram = n_gram( data1, 2 )\n",
    "tri_gram = n_gram( data1, 3 )\n",
    "\n",
    "print( \"* Bi-gram:\", bi_gram )\n",
    "print()\n",
    "print( \"* Tri-gram:\", tri_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## empty space\n",
    "\n",
    "알파벳으로 이루어진 영미권 텍스트 자료의 경우, 구두점(punctuation) 및 띄어쓰기(space)가 잘 되어 있기 때문에, 이를 기준으로 나누는 것 만으로도 token을 도출해 낼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Separated by Space ['punctuation', 'formerly', 'sometimes', 'called', 'pointing', 'is', 'the', 'use', 'of', 'spacing', 'conventional', 'signs', 'and', 'certain', 'typographical', 'devices', 'as', 'aids', 'to', 'the', 'understanding', 'and', 'correct', 'reading', 'of', 'handwritten', 'and', 'printed', 'text', 'whether', 'read', 'silently', 'or', 'aloud1', 'another', 'description', 'is', 'it', 'is', 'the', 'practice', 'action', 'or', 'system', 'of', 'inserting', 'points', 'or', 'other', 'small', 'marks', 'into', 'texts', 'in', 'order', 'to', 'aid', 'interpretation;', 'division', 'of', 'text', 'into', 'sentences', 'clauses', 'etc', 'by', 'means', 'of', 'such', 'marks2']\n"
     ]
    }
   ],
   "source": [
    "data2 = 'Punctuation (formerly sometimes called pointing) is the use of spacing, conventional signs and certain typographical devices as aids to the understanding and correct reading of handwritten and printed text whether read silently or aloud.[1] Another description is, \"It is the practice action or system of inserting points or other small marks into texts in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks.\"[2]'\n",
    "\n",
    "import re \n",
    "data2_lower = data2.lower()\n",
    "data2_clean = re.sub( r\"[\\(\\)\\[\\]\\.\\,\\!\\?\\\"\\']\", \"\", data2_lower )\n",
    "separated_by_space = data2_clean.split()\n",
    "print( \"# Separated by Space\", separated_by_space )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dictionary based\n",
    "\n",
    "용어집이 있는 경우에는 해당 텍스트에서 용어집에 포함된 단어를 추출해 내어 token을 도출해 낼 수 있다. \n",
    "\n",
    "이 때 긴 용어 안에 짧은 용어가 포함되는 경우가 있으므로 긴 용어부터 순차적으로 추출해 내야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score based\n",
    "\n",
    "텍스트에 공백과 같이 기준이 되는 표시도 없고, 용어집도 없다면 token을 도출하는 일은 매우 어려워진다. 특히 표점이 되지 않은 한자의 경우에는 그 어려움이 더 심각하다. \n",
    "\n",
    "여러가지 unsupervised segmentation 방법들이 제안되고 있지만, 계산이 복잡하고 그에 비해 결과가 좋지 않은 편이다. \n",
    "\n",
    "지금까지는 [soynlp](https://github.com/lovit/soynlp)에서 제공하고 있는 응집력 지수(`cohesion`)를 이용한 방법이 가장 효과적인 듯하다. \n",
    "\n",
    "다만 token extraction 단계가 한글에 최적화 되어 있어 그대는 사용하기 어렵다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REF\n",
    "\n",
    "* [Cohesion score + L-Tokenizer. 띄어쓰기가 잘 되어있는 한국어 문서를 위한 unsupervised tokenizer](https://lovit.github.io/nlp/2018/04/09/cohesion_ltokenizer/)\n",
    "* [ratsgo's blog > Cohesion Probability](https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/05/05/cohesion/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
