{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 분석을 위해서는 먼저 어떤 데이터를 분석할 것인가? 어떤 목적을 위해 분석할 것인가?에 대해 염두해 두어야 한다. \n",
    "\n",
    "전자는 분석 대상을 확정하는 것이고, 후자는 분석 모델과 방법을 선택하는데 필수적이다. \n",
    "\n",
    "데이터 분석을 위해 \"데이터\"가 필요하다는 것은, 음식을 만들기 위해 재료가 필요하다는 말처럼 너무나 당연하다. 하지만 적당한 데이터를 얻는 일은 생각처럼 쉽지 않다. \n",
    "\n",
    "분석 목적에 맞는 데이터가 어떤 것인지, 그것을 내가 활용할 수 있는 것인지 많은 조사가 필요하다. \n",
    "\n",
    "좋은 소식이 있다면, 인터넷에서 웹브라우저를 통해 볼 수 있는 자료라면 일단 데이터로 사용할 수 있다는 점이다. 물론 이건 법적인 의미가 아니라 기술적인 의미에서이다. 웹브라우저가 읽어 올 수 있었다면 컴퓨터로 읽어올 수 있는 자료임을 의미하기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "웹에 존재하는 데이터를 수집하는 일을 __Web Scraping__ 혹은 __Web Crawling__이라고 한다. \n",
    "\n",
    "이와 관련된 좋은 library 들이 존재하기 때문에 작업은 생각보다 어렵지 않다. 그러나 웹에서 데이터를 주고 받는 방법이라든지, 웹페이지의 구성 방식 등을 알아야 내가 원하는 데이터를 추출해 낼 수 있다. \n",
    "\n",
    "또 front end에서 페이지를 동적으로 생성하는 동적 페이지의 경우, 데이터가 HTML 속에 존재하지 않기 때문에 headless browser를 이용해야 한다. \n",
    "\n",
    "여기에서는 단순히 공개된 페이지에 담겨 있는 특정 정보를 가져오는 방법에 대해 알아보고자 한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "# from lxml import html\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_get(url):\n",
    "    try:\n",
    "        with closing( get( url, stream=True ) ) as resp:\n",
    "            if is_good_response(resp): return resp.content\n",
    "            else: return None\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format( url, str(e) ) )\n",
    "        return None\n",
    "\n",
    "def is_good_response(resp):\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def select_prescription_links( raw_html ):\n",
    "    \"\"\"\n",
    "    처방 목록 페이지에서 각 처방 주소를 추출함\n",
    "    \"\"\"\n",
    "    rst = []\n",
    "    parsed_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    for a in parsed_html.select('a'):\n",
    "        if a.get('data-target') == 'prescription_view_win':\n",
    "            rst.append(  a.get( 'href' ) )\n",
    "    return rst\n",
    "\n",
    "# 본초 추출\n",
    "def select_medicine_ingredients( raw_html ):\n",
    "    \"\"\"\n",
    "    각 처방 페이지에서 본초 구성 자료를 추출함\n",
    "    \"\"\"\n",
    "    rst = []\n",
    "    parsed_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    for a in parsed_html.select('a'):\n",
    "        if a.get('data-target') == 'medicine_view_win':\n",
    "            rst.append(  a.text  )\n",
    "    return rst\n",
    "\n",
    "def select_medicine_symptoms( raw_html ):\n",
    "    \"\"\"\n",
    "    각 처방 페이지에서 본초 구성 자료를 추출함\n",
    "    \"\"\"\n",
    "    rst = []\n",
    "    parsed_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "    for a in parsed_html.select('a'):\n",
    "        if a.get('data-target') == 'disease_view_win':\n",
    "            rst.append(  a.text  )\n",
    "    return rst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "특허청 한국전통지식포탈 > 전통의료 > 처방 \n",
    "* 2019-02-20 기준\n",
    "* 총 3,278건, 100건씩 33 페이지\n",
    "* http://www.koreantk.com/ktkp2014/prescription/list-by-index.page?pageSize=100&pageNo=1\n",
    "\"\"\"\n",
    "\n",
    "n_pages = 33\n",
    "url_base = \"http://www.koreantk.com\"\n",
    "url_path = \"/ktkp2014/prescription/list-by-index.page\"\n",
    "url_query = \"?pageSize=100&pageNo={}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping & Selecting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Sandbox\n",
    "\n",
    "temp_folder_path = \"../data/_sandbox\"\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists( temp_folder_path ):\n",
    "    os.makedirs( temp_folder_path )\n",
    "    print( \"Folder Created ... {}\".format( temp_folder_path ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/list-by-index.page?pageSize=100&pageNo=1\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/list-by-index.page?pageSize=100&pageNo=2\n",
      "\n",
      "# Prescription Link Lists\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0000001\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0020266\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0000002\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0008499\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0000003\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0004755\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0000004\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0020284\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0015600\n",
      "/ktkp2014/prescription/prescription-view.view?preCd=P0015601\n"
     ]
    }
   ],
   "source": [
    "prescription_links = []\n",
    "\n",
    "for i in range(n_pages)[:2]:\n",
    "    url = url_base + url_path + url_query.format(i+1)\n",
    "    raw_html = simple_get( url )\n",
    "    with open( temp_folder_path + \"/list_{:03d}.html\".format(i), 'w', encoding=\"utf-8\" ) as fl:\n",
    "        fl.write( raw_html.decode('utf-8') )\n",
    "    prescription_links += select_prescription_links( raw_html )\n",
    "    print( \"Web Scraping ... \", url )\n",
    "    time.sleep( 5 )\n",
    "\n",
    "print()\n",
    "print( \"# Prescription Link Lists\")    \n",
    "print( \"\\n\".join( prescription_links[:10] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0000001\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0020266\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0000002\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0008499\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0000003\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0004755\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0000004\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0020284\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0015600\n",
      "Web Scraping ...  http://www.koreantk.com/ktkp2014/prescription/prescription-view.view?preCd=P0015601\n"
     ]
    }
   ],
   "source": [
    "prescriptions = []\n",
    "i = 0\n",
    "\n",
    "for p_url_path in prescription_links[:10]:\n",
    "    url = url_base + p_url_path\n",
    "    raw_html = simple_get(url)\n",
    "    i += 1\n",
    "    with open( temp_folder_path + \"/prescriptions_{:04d}.html\".format(i), 'w', encoding=\"utf-8\" ) as fl:\n",
    "        fl.write( raw_html.decode('utf-8') )\n",
    "    ingr = select_medicine_ingredients( raw_html )\n",
    "    symp = select_medicine_symptoms( raw_html )\n",
    "    prescriptions.append( { \"herbs\": ingr, \"symptoms\": symp } )\n",
    "    print( \"Web Scraping ... \", url )\n",
    "    time.sleep( 5 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separated Data Format ( CSV, TSV )\n",
    "\n",
    "* [numpy.savetxt](https://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = temp_folder_path + \"/prescriptions\"\n",
    "\n",
    "# prescription_herb.csv & prescription_symp.csv\n",
    "separator = \",\"\n",
    "csv_h = open( output_path + \"_herb.csv\", 'w', encoding=\"utf-8\" )\n",
    "csv_s = open( output_path + \"_symp.csv\", 'w', encoding=\"utf-8\" )\n",
    "for p in prescriptions:\n",
    "    csv_h.write(   separator.join( p.get(\"herbs\")      )  + \"\\n\"  )\n",
    "    csv_s.write(   separator.join( p.get(\"symptoms\")   )  + \"\\n\"  )\n",
    "csv_h.close()\n",
    "csv_s.close\n",
    "\n",
    "# prescription_herb.tsf & prescription_symp.tsv\n",
    "separator = \"\\t\"\n",
    "tsv_h = open( output_path + \"_herb.tsv\", 'w', encoding=\"utf-8\" )\n",
    "tsv_s = open( output_path + \"_symp.tsv\", 'w', encoding=\"utf-8\" )\n",
    "for p in prescriptions:\n",
    "    tsv_h.write(   separator.join( p.get(\"herbs\")     )  + \"\\n\"  )\n",
    "    tsv_s.write(   separator.join( p.get(\"symptoms\")  )  + \"\\n\"  )\n",
    "tsv_h.close()\n",
    "tsv_s.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Data Format ( JSON, YAML, XML )\n",
    "\n",
    "* [json](https://docs.python.org/3/library/json.html)\n",
    "* [PyYAML](https://pyyaml.org/wiki/PyYAMLDocumentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "\n",
    "# prescriptions.json\n",
    "json_str = json.dumps( prescriptions, ensure_ascii=False, sort_keys=True, indent=4 )\n",
    "with open( \"../data/kntk_formulas_toy.json\", 'w', encoding=\"utf-8\" ) as fl:\n",
    "    fl.write( json_str )\n",
    "\n",
    "# prescriptions.yaml\n",
    "yaml_str = yaml.dump( prescriptions, allow_unicode=True, indent=4 )\n",
    "with open( output_path + \".yaml\", 'w', encoding=\"utf-8\" ) as fl:\n",
    "    fl.write( yaml_str )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Format for Python Object \n",
    "\n",
    "* [pickle](https://docs.python.org/3/library/pickle.html)\n",
    "* [joblib](https://joblib.readthedocs.io/en/latest/)\n",
    "* [numpy.save](https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# prescription.pickle\n",
    "pickle.dump( prescriptions, open( output_path + \".pickle\", 'wb' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFs\n",
    "\n",
    "\n",
    "* [Practical Introduction to Web Scraping in Python](https://realpython.com/python-web-scraping-practical-introduction/)\n",
    "* [Save and Load Machine Learning Models in Python with scikit-learn](https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
