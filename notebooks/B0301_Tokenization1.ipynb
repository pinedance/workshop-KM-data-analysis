{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization ( or Segmentation )\n",
    "\n",
    "텍스트는 문자(`chr`)의 나열(`string`)이다. 이를 데이터로 사용하기 위해서는 기본 단위로 분절해야 한다. \n",
    "\n",
    "이렇게 분절된 결과를 __token__이라고 하고, 이런 과정을 __tokenization__이라고 한다. \n",
    "\n",
    "때때로 __Segmentation__이라고도 하는데, 긴 글자의 나열을 적절하게 끊는다는 의미로 사용된다. \n",
    "\n",
    "여기에서는 한문으로 이루어진 텍스트를 예시로 하고 있기 때문에 Segmentaion이라고 표현하기로 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmentation은 크게 다음과 같은 방법이 있다.\n",
    "\n",
    "* N-gram\n",
    "* Empty Space based Segmentation\n",
    "* Dictionary based Segmentation\n",
    "* Score based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram \n",
    "\n",
    "문자의 나열이므로 적당한 문자 길이로 자르는 방법을 떠올릴 수 있다. \n",
    "\n",
    "1자씩 나누는 것을 uni-gram(1gram), 2자씩 나누는 것을 bi-gram(2gram), 3자씩 나누는 것을 tri-gram(3gram)이라고 한다. 보통 bi-gram을 가장 많이 사용한다. \n",
    "\n",
    "단, 여기서 gram의 단위는 한 글자인 경우도 있고 한 단어인 경우도 있다. \"나는 학교에 간다.\"라는 문장을 bi-gram으로 나눈다고 하였을 때, gram의 단위에 따라 다음과 같이 결과가 달라지게 된다. \n",
    "\n",
    "* 글자: `['나는', '는_', '_학', '학교', '교에', '에_', '_간', '간다' ]`\n",
    "* 단어: `[(\"나는\", \"학교에\"), (\"학교에\", \"간다\")]`\n",
    "\n",
    "한문의 경우에는 한 글자가 한 단어의 의미를 가지므로 글자를 기준으로 하는가 단어를 기준으로 하는가의 구분이 큰 의미가 없다. 글자 기준의 uni-gram을 이용하면 영어나 한글에서 단어 기준의 uni-gram을 사용하는 것과 유사한 의미를 가지기 때문이다. \n",
    "\n",
    "한의학의 단어들은 2-6글자 사이이므로 bi-gram만으로도 어느정도의 특징을 분석해 낼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雜病篇卷之四 > 內傷 > 勞倦傷治法 > 補中益氣湯 1.17.1\n",
    "# https://mediclassics.kr/books/8/volume/12#content_267\n",
    "data1 = \"治勞役太甚或飮食失節身熱而煩自汗倦怠黃芪 一錢半人參白朮甘草各一錢當歸身陳皮各五分升麻柴胡各三分右剉作一貼水煎服\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# N-gram based Tokenization\n",
      "* Uni-gram:\n",
      " ['治', '勞', '役', '太', '甚', '或', '飮', '食', '失', '節', '身', '熱', '而', '煩', '自', '汗', '倦', '怠', '黃', '芪', ' ', '一', '錢', '半', '人', '參', '白', '朮', '甘', '草', '各', '一', '錢', '當', '歸', '身', '陳', '皮', '各', '五', '分', '升', '麻', '柴', '胡', '各', '三', '分', '右', '剉', '作', '一', '貼', '水', '煎', '服']\n",
      "* Bi-gram:\n",
      " ['治勞', '勞役', '役太', '太甚', '甚或', '或飮', '飮食', '食失', '失節', '節身', '身熱', '熱而', '而煩', '煩自', '自汗', '汗倦', '倦怠', '怠黃', '黃芪', '芪 ', ' 一', '一錢', '錢半', '半人', '人參', '參白', '白朮', '朮甘', '甘草', '草各', '各一', '一錢', '錢當', '當歸', '歸身', '身陳', '陳皮', '皮各', '各五', '五分', '分升', '升麻', '麻柴', '柴胡', '胡各', '各三', '三分', '分右', '右剉', '剉作', '作一', '一貼', '貼水', '水煎', '煎服']\n",
      "* Tri-gram:\n",
      " ['治勞役', '勞役太', '役太甚', '太甚或', '甚或飮', '或飮食', '飮食失', '食失節', '失節身', '節身熱', '身熱而', '熱而煩', '而煩自', '煩自汗', '自汗倦', '汗倦怠', '倦怠黃', '怠黃芪', '黃芪 ', '芪 一', ' 一錢', '一錢半', '錢半人', '半人參', '人參白', '參白朮', '白朮甘', '朮甘草', '甘草各', '草各一', '各一錢', '一錢當', '錢當歸', '當歸身', '歸身陳', '身陳皮', '陳皮各', '皮各五', '各五分', '五分升', '分升麻', '升麻柴', '麻柴胡', '柴胡各', '胡各三', '各三分', '三分右', '分右剉', '右剉作', '剉作一', '作一貼', '一貼水', '貼水煎', '水煎服']\n"
     ]
    }
   ],
   "source": [
    "def n_gram( text, n=2 ):\n",
    "    size = len( text )\n",
    "    grams = [ text[i:i+n] for i in range(size -n+1 ) ]\n",
    "    return grams\n",
    "\n",
    "uni_gram = list( data1 )\n",
    "bi_gram = n_gram( data1, 2 )\n",
    "tri_gram = n_gram( data1, 3 )\n",
    "\n",
    "print( \"# N-gram based Tokenization\" )\n",
    "print( \"* Uni-gram:\\n\", uni_gram )\n",
    "print( \"* Bi-gram:\\n\", bi_gram )\n",
    "print( \"* Tri-gram:\\n\", tri_gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empty Space based Segmentation\n",
    "\n",
    "알파벳으로 이루어진 영미권 텍스트 자료의 경우, 구두점(punctuation) 및 띄어쓰기(space)가 잘 되어 있기 때문에, 이를 기준으로 나누는 것 만으로도 token을 도출해 낼 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Empty Space based Segmentation: \n",
      " ['punctuation', 'formerly', 'sometimes', 'called', 'pointing', 'is', 'the', 'use', 'of', 'spacing', 'conventional', 'signs', 'and', 'certain', 'typographical', 'devices', 'as', 'aids', 'to', 'the', 'understanding', 'and', 'correct', 'reading', 'of', 'handwritten', 'and', 'printed', 'text', 'whether', 'read', 'silently', 'or', 'aloud1', 'another', 'description', 'is', 'it', 'is', 'the', 'practice', 'action', 'or', 'system', 'of', 'inserting', 'points', 'or', 'other', 'small', 'marks', 'into', 'texts', 'in', 'order', 'to', 'aid', 'interpretation;', 'division', 'of', 'text', 'into', 'sentences', 'clauses', 'etc', 'by', 'means', 'of', 'such', 'marks2']\n"
     ]
    }
   ],
   "source": [
    "data2 = 'Punctuation (formerly sometimes called pointing) is the use of spacing, conventional signs and certain typographical devices as aids to the understanding and correct reading of handwritten and printed text whether read silently or aloud.[1] Another description is, \"It is the practice action or system of inserting points or other small marks into texts in order to aid interpretation; division of text into sentences, clauses, etc., by means of such marks.\"[2]'\n",
    "\n",
    "import re \n",
    "data2_lower = data2.lower()\n",
    "data2_clean = re.sub( r\"[\\(\\)\\[\\]\\.\\,\\!\\?\\\"\\']\", \"\", data2_lower )\n",
    "separated_by_space = data2_clean.split()\n",
    "print( \"# Empty Space based Segmentation: \\n\", separated_by_space )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary based Segmentation\n",
    "\n",
    "용어집이 있는 경우에는 해당 텍스트에서 용어집에 포함된 단어를 추출해 내어 token을 도출해 낼 수 있다. \n",
    "\n",
    "이 때 긴 용어 안에 짧은 용어가 포함되는 경우가 있으므로 긴 용어부터 순차적으로 추출해 내야 한다. \n",
    "\n",
    "용어집에 炙甘, 炙甘草, 甘草, 甘草湯, 草湯, 炙甘草湯이 있을 때, \"...炙甘草湯...\"이라는 문장에서 어떻게 tokne을 도출해 낼 것인지 생각해 보아야 한다. 크게 다음과 같은 경우가 모두 가능하다. \n",
    "\n",
    "* 겹침 허용 함                   : `['甘草', '炙甘', '草湯', '炙甘草', '甘草湯', '炙甘草湯']`\n",
    "* 겹침 허용 안함, 짧은 용어 우선 : `['炙甘', '草湯']`\n",
    "* 겹침 허용 안함, 긴 용어 우선   : `['炙甘草湯']`\n",
    "\n",
    "겹침을 허용하면 token 추출이 단순해 지는 반면, 부정확한 내용들이 포함되게 된다. 겹침을 허용하지 않고 긴 용어를 우선 추출하면 추출 과정이 다소 복잡해지지만 비교적 정확한 token을 얻을 수 있다. \n",
    "\n",
    "그러나 찾고자 하는 적확한 용어(예시에서 '炙甘草湯')가 용어집에 없는 경우에는 오히려 겹침을 허용하는 것이 맥락을 이해하는데 더 도움을 줄 수 있다. \n",
    "\n",
    "* 겹침 허용 함                   : `['甘草', '炙甘', '草湯', '炙甘草', '甘草湯']`\n",
    "* 겹침 허용 안함, 짧은 용어 우선 : `['炙甘', '草湯']`\n",
    "* 겹침 허용 안함, 긴 용어 우선   : `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dictionary based Tokenization (index, term, len)\n",
      "(1, '勞役', 2)\n",
      "(10, '身熱', 2)\n",
      "(13, '煩', 1)\n",
      "(14, '自汗', 2)\n",
      "(16, '倦怠', 2)\n",
      "(18, '黃芪', 2)\n",
      "(24, '人參', 2)\n",
      "(26, '白朮', 2)\n",
      "(28, '甘草', 2)\n",
      "(33, '當歸身', 3)\n",
      "(33, '當歸', 2)\n",
      "(36, '陳皮', 2)\n",
      "(41, '升麻', 2)\n",
      "(43, '柴胡', 2)\n"
     ]
    }
   ],
   "source": [
    "# 겹침을 허용한 추출의 예\n",
    "\n",
    "voc_s = ['勞役', '身熱', '煩', '自汗', '倦怠']\n",
    "voc_h = ['黃芪', '人參', '白朮', '甘草', '當歸身', '當歸', '陳皮', '升麻', '柴胡']\n",
    "\n",
    "data1 = \"治勞役太甚或飮食失節身熱而煩自汗倦怠黃芪 一錢半人參白朮甘草各一錢當歸身陳皮各五分升麻柴胡各三分右剉作一貼水煎服\"\n",
    "\n",
    "import re\n",
    "\n",
    "terms_ = []\n",
    "for v in voc_s + voc_h :\n",
    "    i = data1.find( v )\n",
    "    if i < 0 : continue\n",
    "    terms_.append( ( i, v, len(v) ) )\n",
    "        \n",
    "term = sorted( terms_, key=lambda x: (x[0], -x[2]) )\n",
    "\n",
    "print( \"# Dictionary based Tokenization (index, term, len)\")\n",
    "for t in term: print(t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
